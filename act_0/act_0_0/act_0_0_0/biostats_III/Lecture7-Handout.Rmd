---
title: "Lecture7 Handout"
author: "Elizabeth Colantuoni"
date: "2/14/2021"
header-includes:
   - \usepackage{undertilde}
output: pdf_document

---

# I. Objectives

Upon completion of this session, you will be able to do the following:

* Use vector notation to specify the multiple linear regression model (*REVIEW*)

* Derive the least squares estimators using vector notation (*REVIEW*)
    
* Give a geometric explanation of least squares

* Derive the exact (under Gaussian model) or asymptotic distribution of the major regression results in vector notation: regression coefficients, linear functions thereof, predicted values, residuals

* Design and implement a simulation study to evaluate the properties of regression coefficients in MLR when the Gaussian model for residuals doesn't hold

# II. REVIEW: MLR in vector notation

In this section, we will walk back through the derivations of $\utilde{\hat{\beta}}$ but expressing the regression models using vector and matrix notation.

Consider the following structure for our regression problem for $i = 1, ..., n$:

\begin{center}
$Y_i = \beta_0 + X_{1i} \beta_1 + X_{2i} \beta_2 + ... + X_{pi} \beta_p + \epsilon_i$
\end{center}

where $\epsilon_i \text{ are independently distributed as } N(0,\sigma^2)$ 

We can then stack each individuals data into a table structure:

\begin{center}
\begin{tabular}{cccccccccccccc}
$\underline{i}$ & & & & & & & & & & & & & \\
1 & $Y_1$ & = & $1 \times \beta_0$ & + & $X_{11} \beta_1$ & + & $X_{21} \beta_2$ & + & $ ... $ & + & $X_{p1} \beta_p$ & = & $\epsilon_1$ \\
2 & $Y_2$ & = & $1 \times \beta_0$ & + & $X_{12} \beta_1$ & + & $X_{22} \beta_2$ & + & $ ... $ & + & $X_{p2} \beta_p$ & = & $\epsilon_2$ \\
. & & & & & & & & & & & & & \\
. & & & & & & & & & & & & & \\
. & & & & & & & & & & & & & \\
n & $Y_n$ & = & $1 \times \beta_0$ & + & $X_{1n} \beta_1$ & + & $X_{2n} \beta_2$ & + & $ ... $ & + & $X_{pn} \beta_p$ & = & $\epsilon_n$ \\
\end{tabular}
\end{center}

We can then think about creating vectors that contain the same type of information for each element of our model:

$$\left( \begin{array}{c} Y_1 \\ Y_2 \\ . \\ . \\ . \\ Y_p \end{array} \right) \begin{array}{c} = \\ = \\ . \\ . \\ . \\ = \end{array} 
\left( \begin{array}{c} 1 \times \beta_0 \\ 1 \times \beta_0 \\ . \\ . \\ . \\ 1 \times \beta_0 \end{array} \right) \begin{array}{c} + \\ + \\ . \\ . \\ . \\ + \end{array} \left( \begin{array}{c} X_{11} \beta_1 \\ X_{12} \beta_1 \\ . \\ . \\ . \\ X_{1n} \beta_1 \end{array} \right) \begin{array}{c} + \\ + \\ . \\ . \\ . \\ + \end{array} \left( \begin{array}{c} X_{21} \beta_2 \\ X_{22} \beta_2 \\ . \\ . \\ . \\ X_{2n} \beta_2 \end{array} \right) \begin{array}{c} + \\ + \\ . \\ . \\ . \\ + \end{array} \begin{array}{c} ... \\ ... \\ . \\ . \\ . \\ ... \end{array} \begin{array}{c} + \\ + \\ . \\ . \\ . \\ + \end{array} \left( \begin{array}{c} X_{p1} \beta_p \\ X_{p2} \beta_p \\ . \\ . \\ . \\ X_{pn} \beta_p \end{array} \right) \begin{array}{c} = \\ = \\ . \\ . \\ . \\ = \end{array} \left( \begin{array}{c} \epsilon_1 \\ \epsilon_2 \\ . \\ . \\ . \\ \epsilon_n \end{array} \right)$$

We can then express the system of equations in vector notation:

\begin{center}
\begin{tabular}{ccccccccccccc}
$\utilde{Y}$ & = & $\utilde{1} \beta_0$ & + & $\utilde{X}_1 \beta_1$ & + & $\utilde{X}_2 \beta_2$ & + & ... & + & $\utilde{X}_p \beta_p$ & = & $\utilde{\epsilon}$ \\
$n \times 1$& & $n \times 1$ & & $n \times 1$ & & $n \times 1$ & & & & $n \times 1$ & & $n \times 1$\\
\end{tabular}
\end{center}

Further, we can append the column vectors represented by $\utilde{1}$, $\utilde{X}_1$, ..., $\utilde{X}_p$ into a matrix and multiply this matrix with the vector of regression coefficients:

\begin{center}
$$\utilde{Y} = \left( \utilde{1}, \utilde{X}_1, \utilde{X}_2, ..., \utilde{X}_p \right) \left( \begin{array}{c} \beta_0 \\ \beta_1 \\ \beta_2 \\ . \\ . \\ . \\ \beta_p \end{array} \right) + \utilde{\epsilon}$$
$$\left( \begin{array}{c} Y_1 \\ Y_2 \\ . \\ . \\ . \\ Y_n \end{array} \right) = \left[ \begin{array}{cccccc} 1 & X_{11} & X_{21} & ... & X_{p1} \\ 1 & X_{12} & X_{22} & ... & X_{p2} \\ . & . & . & ... & . \\
. & . & . & ... & . \\ . & . & . & ... & . \\ 1 & X_{1n} & X_{2n} & ... & X_{pn} \end{array} \right] \left( \begin{array}{c} \beta_0 \\ \beta_1 \\ \beta_2 \\ . \\ . \\ . \\ \beta_p \end{array} \right) + \left( \begin{array}{c} \epsilon_1 \\ \epsilon_2 \\ . \\ . \\ . \\ \epsilon_n \end{array} \right)$$
\end{center}

\newpage

This organization of the model leaves us with the following matrix representation of the MLR:

\begin{center}
\begin{tabular}{cccccc}
$\utilde{Y}$ & = & $X$ &  $\utilde{\beta}$ & + & $\utilde{\epsilon}$ \\
& & & & & \\
$n \times 1$ & & $n \times (p+1)$ &  $(p+1) \times 1$ & & $n \times 1$ \\
\end{tabular}
\end{center}

How do we express the distribution of $\utilde{\epsilon}$?

## A. Multivariate Gaussian distribution

The multivariate Gaussian disribution describes the marginal and joint distribution of 2 or more Gaussian random variables.  In matrix notation, we can define the multivariate Gaussian distribution as:

$$\utilde{Y} \sim MVN(\utilde{\mu}, V)$$

where, 

$$\utilde{Y} = \left( \begin{array}{c} Y_1 \\ Y_2 \\ . \\ . \\ . \\ Y_n \end{array} \right) \text{, } \utilde{\mu} = \left( \begin{array}{c} \mu_1 \\ \mu_2 \\ . \\ . \\ . \\ \mu_n \end{array} \right) \text{, and } V = \left[ \begin{array}{cccccc} v_{11} & v_{12} & . & . & . & v_{1n} \\
v_{21} & v_{22} & . & . & . & v_{2n} \\
. & . & . & & & . \\ . & . & & . & & . \\ . & . & & & . & . \\ 
v_{n1} & v_{n2} & . & . & . & v_{nn} \end{array} \right]$$

with $v_{ii} = Var(Y_i)$ and $v_{ij} = Cov(Y_i,Y_j)$.

Given our MLR under the assumption that $\epsilon_i iid N(0,\sigma^2)$ then we have:

  + $E(Y_i) = \mu_i = X_i \utilde{\beta}$ where $X_i = (1, X_{1i}, X_{2i}, ..., X_{pi}) \text{ (the ith row of X)}$
  
  + $Var(Y_i) = \sigma^2$ and $Cov(Y_i, Y_j) = 0$

And we can express the multivariate normal distribution for $\utilde{\epsilon}$ as $\utilde{\epsilon} \sim N(\utilde{0},\sigma^2 I)$ where $I$ is the identity matrix with $1s$ on the diagonal elements and $0s$ on the off-diagonal elements.

## B. Maximum likelihood estimation = least squares using vector notation

Using vector notation, our MLR is:

$$\utilde{Y} = X\utilde{\beta} + \utilde{\epsilon} \text{, } \utilde{\epsilon} \sim MVN(\utilde{0},\sigma^2 I)$$
In the remainder of this section, I will drop the $\utilde{.}$ so you should assume $Y$ and $\beta$ are $n \times 1$ and $(p+1) \times 1$ vectors, respectively, and $X$ is the $n \times (p+1)$ design matrix.

Our goal is to select estimates of $\beta$ and $\sigma^2$ to minimize $\displaystyle\sum_{i=1}^n r_i(\hat{\beta}) = \displaystyle\sum_{i=1}^n (y_i - X_i \beta)^2$.

Using the vector notation, we can express the sums of squared residuals as:

$$\displaystyle\sum_{i=1}^n (y_i - X_i \beta)^2 = (Y - X\beta)^{\shortmid} (Y-X\beta) = ||Y-X\beta||^2$$

where $Y^{\shortmid}$ is the transpose of $Y$:  if $Y$ is a $n \times 1$ vector, then $Y^{\shortmid}$ is a $1 \times n$ vector.  If $X$ is a $n \times (p+1)$ matrix, then $X^{\shortmid}$ is a $(p+1) \times n$ matrix.

The score equations for $\beta$ can be written as:

$$U_{\beta}(\beta) = \frac{\partial}{\partial \beta} (Y - X\beta)^{\shortmid} (Y - X\beta) = X^\shortmid (Y - X\beta) = 0$$

Solving the score equations for $\beta$, we have:

\begin{center}
\begin{tabular}{rcl}
$X^\shortmid Y - X^\shortmid X \beta$ & =&  0 \\
& & \\
$X^\shortmid Y$ & = & $(X^\shortmid X) \beta$ \\
& & \\
$(X^\shortmid X)^{-1} X^\shortmid Y$ & = & $\hat{\beta}$\\
\end{tabular}
\end{center}

## C. Major regression results in vector notation with distributions

Now that we have an expression for the MLE of $\beta$ in the MLR expressed using vector notation.  In this section, we will provide definitions of key regression results in vector notation and derive distributions of key results under the Gaussian assumption.

### 1. Distribution of $\hat{\beta}$

The MLE of $\beta$ is given by:  $\hat{\beta} = (X^\shortmid X)^{-1} X^\shortmid Y = AY$ where $A = (X^\shortmid X)^{-1} X^\shortmid$.

We know that $Y \sim MVN(X\beta,\sigma^2I)$, so $AY \sim MVN(AX\beta, \sigma^2 A I A^\shortmid)$.

Plugging in $A = (X^\shortmid X)^{-1} X^\shortmid$, we have:

\begin{center}
\begin{tabular}{rcl}
$AX\beta$ &  = &  $(X^\shortmid X)^{-1} X^\shortmid X\beta$ \\
& & \\
& = &  $\beta$ \\
& & \\
& & \\
$A I A^\shortmid$ & = & $(X^\shortmid X)^{-1} X^\shortmid ((X^\shortmid X)^{-1} X^\shortmid)^\shortmid$ \\
& & \\
& = & $(X^\shortmid X)^{-1} X^\shortmid X ((X^\shortmid X)^{-1})^\shortmid$ \\
& & \\
& = & $(X^\shortmid X)^{-1} (X^\shortmid X) (X^\shortmid X)^{-1}$ \\
& & \\
& = & $(X^\shortmid X)^{-1}$ \\
\end{tabular}
\end{center}

So that:  $\hat{\beta} \sim MVN(\beta, \sigma^2 (X^\shortmid X)^{-1})$

   +  Does this look familiar?

   +  NOTE:  $X^\shortmid X$ and $(X^\shortmid X)^{-1}$ are symmetric and $X^\shortmid X (X^\shortmid X)^{-1} = (X^\shortmid X)^{-1} X^\shortmid X = I_{n \times n}$

\newpage

### 2. Predicted values

In vector notation, the predicted values are:

\begin{center}
\begin{tabular}{rcl}
$\hat{Y}$ &  = &  $X\hat{\beta}$ \\
& & \\
& = & $X (X^\shortmid X)^{-1} X^\shortmid Y$ \\
& & \\
& = & $\left[X (X^\shortmid X)^{-1} X^\shortmid\right]Y$ \\
& = & $HY$
\end{tabular}
\end{center}

The matrix $H$ is referred to as the *hat-matrix*.

To obtain the predicted value for $Y_i$, you multiple the $ith$ row of $H$ with the vector $Y$, such that the predicted value for each $i$ is a weighted average of the observed $y$ with the weights defined by the $ith$ row of $H$. **See HW1**

The distribution of $\hat{Y}$ is multivariate normal with mean $E(\hat{Y}) = E(X\hat{\beta}) = X\beta$ and variance:

\begin{center}
\begin{tabular}{rcl}
$Var(\hat{Y})$ & = & $Var(X\hat{\beta})$ \\
& & \\
& = & $X Var(\hat{\beta}) X^\shortmid$ \\
& & \\
& = & $\sigma^2 X (X^\shortmid X)^{-1} X^\shortmid$ \\
& & \\
& = & $\sigma^2 H$ \\
\end{tabular}
\end{center}

#### i. Properties of the Hat matrix

The Hat matrix has some unique properties.

1. $H$ is symmetric:

\begin{center}
\begin{tabular}{rcl}
$H^\shortmid$ & = & $\left[ X (X^\shortmid X)^{-1} X^\shortmid \right]^\shortmid$ \\
& & \\
& = & $X (X^\shortmid X) ^{-1 \shortmid} X^\shortmid$\\
& & \\ 
& = & $X (X^\shortmid X)^{-1} X^\shortmid$ \\
& & \\
& = & $H$ \\
\end{tabular}
\end{center}

2. H is idempotent, i.e. $H H = H$

\begin{center}
\begin{tabular}{rcl}
$H H$ & = & $\left[X (X^\shortmid X)^{-1} X^\shortmid \right] \left[X (X^\shortmid X)^{-1} X^\shortmid \right]$ \\
& & \\
& = & $X (X^\shortmid X)^{-1} X^\shortmid X (X^\shortmid X)^{-1} X^\shortmid$ \\
& & \\
& = & $X (X^\shortmid X)^{-1} X^\shortmid$ \\
& & \\
& = & $H$ \\
\end{tabular}
\end{center}

### 3. Residuals

In vector notation, the residuals are: $\hat{R} = Y - \hat{Y} = Y - HY = (I - H)Y$.

The distribution of the residuals will be multivariate normal with mean: $E(Y - \hat{Y}) = X\beta - X\beta = 0$ with variance: $Var(Y - \hat{Y}) = Var((I-H)Y) = \sigma^2(I-H)$.  Can you derive the $Var(\hat{R})$ yourself?

### 4. Relationship between $\hat{Y}$ and $R$

We can show that the predicted values and residuals are independent.  Does this make sense?

\begin{center}
\begin{tabular}{rcl}
$Cov(\hat{Y},\hat{R})$ & = & $E\left[HY\left((I-H)Y\right)^\shortmid \right]$ \\
& & \\
& = & $H E\left[Y Y^\shortmid \right] (I-H)$ \\
& & \\
& = & $H \sigma^2 I (I-H)$ \\
& & \\
& = & $\sigma^2 H (I-H) = 0$ \\
\end{tabular}
\end{center}

## D. Geometry of least squares

See lecture slides for motivation of geometry of least squares.

# III. Normality of regression coefficients

See extra Rmarkdown file for the simulation set up and your group exercise.
